{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZmlmB+qmGJr/iUNTr5rub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharath-0824/Natural-Language-Processing-Tasks/blob/main/Task_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lg7BnvVBrwI",
        "outputId": "d1df2bc2-141d-44d4-e66d-996d417c2942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is\n",
            "he is well\n",
            "he is well known\n",
            "he is well known difficulties\n",
            "he is well known difficulties to\n",
            "he is well known difficulties to obtain\n",
            "he is well known difficulties to obtain satisfactory\n",
            "he is well known difficulties to obtain satisfactory financing\n",
            "he is well known difficulties to obtain satisfactory financing and\n",
            "he is well known difficulties to obtain satisfactory financing and close\n",
            "he is well known difficulties to obtain satisfactory financing and close to\n",
            "he is well known difficulties to obtain satisfactory financing and close to two\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from the\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from the previous\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from the previous record\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from the previous record 119\n",
            "he is well known difficulties to obtain satisfactory financing and close to two mln stg from the previous record 119 ,\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, reuters\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import FreqDist, ngrams\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt_tab')\n",
        "sents = reuters.sents()\n",
        "stop_word = set(stopwords.words('english'))\n",
        "string.punctuation = string.punctuation + ' \" ' + ' \" ' + ' - ' + ' _ '\n",
        "removal_list = list(stop_word) + list(string.punctuation) + ['\\t', 'rt']\n",
        "unigram = []\n",
        "bigram = []\n",
        "trigram = []\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = list(map(lambda x: x.lower(), sentence))\n",
        "    for word in sentence:\n",
        "        if word == '.':\n",
        "            sentence.remove(word)\n",
        "        else:\n",
        "            unigram.append(word)\n",
        "            tokenized_text.append(word)\n",
        "    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True)))\n",
        "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))\n",
        "def remove_stopwords(x):\n",
        "    y = []\n",
        "    for item in x:\n",
        "        if isinstance(item, tuple):\n",
        "            count = 0\n",
        "            for word in item:\n",
        "                if word not in removal_list:\n",
        "                    count = 1\n",
        "                    break\n",
        "            if count == 1:\n",
        "                y.append(item)\n",
        "        else:\n",
        "            if item not in removal_list:\n",
        "                y.append(item)\n",
        "    return y\n",
        "\n",
        "unigram = remove_stopwords(unigram)\n",
        "bigram = remove_stopwords(bigram)\n",
        "trigram = remove_stopwords(trigram)\n",
        "freq_uni = FreqDist(unigram)\n",
        "freq_bi = FreqDist(bigram)\n",
        "freq_tri = FreqDist(trigram)\n",
        "d = defaultdict(Counter)\n",
        "for a, b, c in freq_tri:\n",
        "    if (a is not None) and (b is not None) and (c is not None):\n",
        "        d[a, b][c] += freq_tri[a, b, c]\n",
        "def pick_word(counter):\n",
        "    \"choose a random element\"\n",
        "    return random.choice(list(counter.elements()))\n",
        "prefix = \"he\", \"is\"\n",
        "print(\" \".join(prefix))\n",
        "s = \" \".join(prefix)\n",
        "for i in range(19):\n",
        "    if prefix in d:\n",
        "        suffix = pick_word(d[prefix])\n",
        "        s = s + ' ' + suffix\n",
        "        print(s)\n",
        "        prefix = prefix[1], suffix\n",
        "    else:\n",
        "        print(f\"Prefix {prefix} not found in dictionary. Stopping text generation.\")\n",
        "        break"
      ]
    }
  ]
}